{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thomas/env/py3.10-LLMs/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "(…)cebook/opt-1.3b/resolve/main/config.json: 100%|████████████████████████████| 653/653 [00:00<00:00, 1.18MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-10-18 15:02:54,545] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pytorch_model.bin: 100%|██████████████████████████████████████████████████| 2.63G/2.63G [04:18<00:00, 10.2MB/s]\n",
      "(…)1.3b/resolve/main/generation_config.json: 100%|█████████████████████████████| 137/137 [00:00<00:00, 630kB/s]\n",
      "(…)-1.3b/resolve/main/tokenizer_config.json: 100%|████████████████████████████| 685/685 [00:00<00:00, 3.43MB/s]\n",
      "(…)acebook/opt-1.3b/resolve/main/vocab.json: 100%|██████████████████████████| 899k/899k [00:00<00:00, 4.53MB/s]\n",
      "(…)acebook/opt-1.3b/resolve/main/merges.txt: 100%|██████████████████████████| 456k/456k [00:00<00:00, 5.26MB/s]\n",
      "(…).3b/resolve/main/special_tokens_map.json: 100%|█████████████████████████████| 441/441 [00:00<00:00, 885kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10])\n",
      "{'input_ids': tensor([[    2,   133,  2119,  6219, 23602, 13855,    81,     5, 22414,  2335]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "OPT = AutoModelForCausalLM.from_pretrained(\"facebook/opt-1.3b\", load_in_8bit=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-1.3b\")\n",
    "\n",
    "inp = \"The quick brown fox jumps over the lazy dog\"\n",
    "inp_tokenized = tokenizer(inp, return_tensors=\"pt\")\n",
    "print(inp_tokenized['input_ids'].size())\n",
    "print(inp_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the model and tokenizer using AutoModelForCausalLM and AutoTokenizer, respectively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " the model and tokenizer using AutoModelForCausalLM and AutoTokenizer, respectively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**model's architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPTModel(\n",
      "  (decoder): OPTDecoder(\n",
      "    (embed_tokens): Embedding(50272, 2048, padding_idx=1)\n",
      "    (embed_positions): OPTLearnedPositionalEmbedding(2050, 2048)\n",
      "    (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "    (layers): ModuleList(\n",
      "      (0-23): 24 x OPTDecoderLayer(\n",
      "        (self_attn): OPTAttention(\n",
      "          (k_proj): Linear8bitLt(in_features=2048, out_features=2048, bias=True)\n",
      "          (v_proj): Linear8bitLt(in_features=2048, out_features=2048, bias=True)\n",
      "          (q_proj): Linear8bitLt(in_features=2048, out_features=2048, bias=True)\n",
      "          (out_proj): Linear8bitLt(in_features=2048, out_features=2048, bias=True)\n",
      "        )\n",
      "        (activation_fn): ReLU()\n",
      "        (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear8bitLt(in_features=2048, out_features=8192, bias=True)\n",
      "        (fc2): Linear8bitLt(in_features=8192, out_features=2048, bias=True)\n",
      "        (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(OPT.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the embeedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer:\t Embedding(50272, 2048, padding_idx=1)\n",
      "Size:\t torch.Size([1, 10, 2048])\n",
      "Output:\t tensor([[[-0.0407,  0.0519,  0.0574,  ..., -0.0263, -0.0355, -0.0260],\n",
      "         [-0.0371,  0.0220, -0.0096,  ...,  0.0265, -0.0166, -0.0030],\n",
      "         [-0.0455, -0.0236, -0.0121,  ...,  0.0043, -0.0166,  0.0193],\n",
      "         ...,\n",
      "         [ 0.0007,  0.0267,  0.0257,  ...,  0.0622,  0.0421,  0.0279],\n",
      "         [-0.0126,  0.0347, -0.0352,  ..., -0.0393, -0.0396, -0.0102],\n",
      "         [-0.0115,  0.0319,  0.0274,  ..., -0.0472, -0.0059,  0.0341]]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "embedded_input = OPT.model.decoder.embed_tokens(inp_tokenized['input_ids'])\n",
    "print(\"Layer:\\t\", OPT.model.decoder.embed_tokens)\n",
    "print(\"Size:\\t\", embedded_input.size())\n",
    "print(\"Output:\\t\", embedded_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the positional encoding component utilizes the attention masks to generate a vector that imparts a sense of positioning within the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer:\t OPTLearnedPositionalEmbedding(2050, 2048)\n",
      "Size:\t torch.Size([1, 10, 2048])\n",
      "Output:\t tensor([[[-8.1406e-03, -2.6221e-01,  6.0768e-03,  ...,  1.7273e-02,\n",
      "          -5.0621e-03, -1.6220e-02],\n",
      "         [-8.0585e-05,  2.5000e-01, -1.6632e-02,  ..., -1.5419e-02,\n",
      "          -1.7838e-02,  2.4948e-02],\n",
      "         [-9.9411e-03, -1.4978e-01,  1.7557e-03,  ...,  3.7117e-03,\n",
      "          -1.6434e-02, -9.9087e-04],\n",
      "         ...,\n",
      "         [ 3.6979e-04, -7.7454e-02,  1.2955e-02,  ...,  3.9330e-03,\n",
      "          -1.1642e-02,  7.8506e-03],\n",
      "         [-2.6779e-03, -2.2446e-02, -1.6754e-02,  ..., -1.3142e-03,\n",
      "          -7.8583e-03,  2.0096e-02],\n",
      "         [-8.6288e-03,  1.4233e-01, -1.9012e-02,  ..., -1.8463e-02,\n",
      "          -9.8572e-03,  8.7662e-03]]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "embed_pos_input = OPT.model.decoder.embed_positions(inp_tokenized['attention_mask'])\n",
    "print(\"Layer:\\t\", OPT.model.decoder.embed_positions)\n",
    "print(\"Size:\\t\", embed_pos_input.size())\n",
    "print(\"Output:\\t\", embed_pos_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, the self-attention component! We use the first layer’s self-attention component by indexing through the layers and accessing the .self_attn method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer:\t OPTAttention(\n",
      "  (k_proj): Linear8bitLt(in_features=2048, out_features=2048, bias=True)\n",
      "  (v_proj): Linear8bitLt(in_features=2048, out_features=2048, bias=True)\n",
      "  (q_proj): Linear8bitLt(in_features=2048, out_features=2048, bias=True)\n",
      "  (out_proj): Linear8bitLt(in_features=2048, out_features=2048, bias=True)\n",
      ")\n",
      "Size:\t torch.Size([1, 10, 2048])\n",
      "Output:\t tensor([[[-0.0119, -0.0110,  0.0056,  ...,  0.0094,  0.0013,  0.0093],\n",
      "         [-0.0119, -0.0110,  0.0056,  ...,  0.0095,  0.0013,  0.0093],\n",
      "         [-0.0119, -0.0110,  0.0056,  ...,  0.0095,  0.0013,  0.0093],\n",
      "         ...,\n",
      "         [-0.0119, -0.0110,  0.0056,  ...,  0.0095,  0.0013,  0.0093],\n",
      "         [-0.0119, -0.0110,  0.0056,  ...,  0.0095,  0.0013,  0.0093],\n",
      "         [-0.0119, -0.0110,  0.0056,  ...,  0.0095,  0.0013,  0.0093]]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<MatMul8bitLtBackward>)\n"
     ]
    }
   ],
   "source": [
    "embed_position_input = embedded_input + embed_pos_input\n",
    "hidden_states, _, _ = OPT.model.decoder.layers[0].self_attn(embed_position_input)\n",
    "print(\"Layer:\\t\", OPT.model.decoder.layers[0].self_attn)\n",
    "print(\"Size:\\t\", hidden_states.size())\n",
    "print(\"Output:\\t\", hidden_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Encoder-Decoder Architecture"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Encoder-Decoder architecture](TheEncoder-Decorder-Architecture.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading BART model for inspection the architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(…)book/bart-large/resolve/main/config.json: 100%|████████████████████████| 1.63k/1.63k [00:00<00:00, 3.15MB/s]\n",
      "pytorch_model.bin: 100%|██████████████████████████████████████████████████| 1.02G/1.02G [01:39<00:00, 10.2MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BartModel(\n",
      "  (shared): Embedding(50265, 1024, padding_idx=1)\n",
      "  (encoder): BartEncoder(\n",
      "    (embed_tokens): Embedding(50265, 1024, padding_idx=1)\n",
      "    (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
      "    (layers): ModuleList(\n",
      "      (0-11): 12 x BartEncoderLayer(\n",
      "        (self_attn): BartAttention(\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (activation_fn): GELUActivation()\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (decoder): BartDecoder(\n",
      "    (embed_tokens): Embedding(50265, 1024, padding_idx=1)\n",
      "    (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
      "    (layers): ModuleList(\n",
      "      (0-11): 12 x BartDecoderLayer(\n",
      "        (self_attn): BartAttention(\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (activation_fn): GELUActivation()\n",
      "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): BartAttention(\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "BART = AutoModel.from_pretrained(\"facebook/bart-large\")\n",
    "print(BART)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are already familiar with most of the layers in the BART model. The model is comprised of both encoder and decoder components, with each component consisting of 12 layers. Additionally, The decoder component, in particular, contains an additional **encoder_attn** layer, referred to as cross-attention. The cross-attention component will condition the decoder’s output based on the encoder representations.\n",
    "\n",
    "We can use the fine-tuned version of this model for summarization using the Transformer’s pipeline functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(…)/bart-large-cnn/resolve/main/config.json: 100%|████████████████████████| 1.58k/1.58k [00:00<00:00, 2.96MB/s]\n",
      "pytorch_model.bin: 100%|██████████████████████████████████████████████████| 1.63G/1.63G [02:39<00:00, 10.2MB/s]\n",
      "(…)-cnn/resolve/main/generation_config.json: 100%|█████████████████████████████| 363/363 [00:00<00:00, 715kB/s]\n",
      "(…)k/bart-large-cnn/resolve/main/vocab.json: 100%|██████████████████████████| 899k/899k [00:00<00:00, 3.80MB/s]\n",
      "(…)k/bart-large-cnn/resolve/main/merges.txt: 100%|██████████████████████████| 456k/456k [00:00<00:00, 11.6MB/s]\n",
      "(…)rt-large-cnn/resolve/main/tokenizer.json: 100%|████████████████████████| 1.36M/1.36M [00:00<00:00, 3.93MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bennett and Gaga became fast friends and close collaborators. They recorded two albums together, 2014's \"Cheek to Cheek\" and 2021's \"Love for Sale\"\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "sum = summarizer(\"\"\"Gaga was best known in the 2010s for pop hits like “Poker Face” and avant-garde experimentation on albums like “Artpop,” and Bennett, a singer who mostly stuck to standards, was in his 80s when the pair met. And yet Bennett and Gaga became fast friends and close collaborators, which they remained until Bennett’s death at 96 on Friday. They recorded two albums together, 2014’s “Cheek to Cheek” and 2021’s “Love for Sale,” which both won Grammys for best traditional pop vocal album.\"\"\", min_length=20, max_length=50)\n",
    "print(sum[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Encoder-Only Architecture\n",
    "\n",
    "![BERT](Encoder-Only.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BERT paper (or an improved variant like RoBERTa) introduced a widely recognized pre-trained model that significantly improved the state-of-the-art scores on numerous NLP tasks. The model undergoes pre-training with two learning objectives:\n",
    "\n",
    "1. Masked Language Modeling: masking random tokens from the input and attempting to predict them.\n",
    "2. Next Sentence Prediction: Present sentences in pairs and assess the likelihood of the second sentence in the subsequent sequence of the first sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(…)rt-base-uncased/resolve/main/config.json: 100%|████████████████████████████| 570/570 [00:00<00:00, 1.11MB/s]\n",
      "model.safetensors: 100%|████████████████████████████████████████████████████| 440M/440M [00:43<00:00, 10.2MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertModel(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): BertEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0-11): 12 x BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): BertPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "BERT = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "print(BERT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code uses the fine-tuned version of the BERT model for sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(…)cased-sentiment/resolve/main/config.json: 100%|████████████████████████████| 953/953 [00:00<00:00, 5.02MB/s]\n",
      "pytorch_model.bin: 100%|████████████████████████████████████████████████████| 669M/669M [01:05<00:00, 10.2MB/s]\n",
      "(…)iment/resolve/main/tokenizer_config.json: 100%|███████████████████████████| 39.0/39.0 [00:00<00:00, 317kB/s]\n",
      "(…)uncased-sentiment/resolve/main/vocab.txt: 100%|██████████████████████████| 872k/872k [00:00<00:00, 6.89MB/s]\n",
      "(…)ent/resolve/main/special_tokens_map.json: 100%|█████████████████████████████| 112/112 [00:00<00:00, 226kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': '5 stars', 'score': 0.8550480604171753}]\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline(\"text-classification\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "lbl = classifier(\"\"\"This restaurant is awesome.\"\"\")\n",
    "\n",
    "print(lbl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Decoder-Only Architecture\n",
    "\n",
    "![decoder-only architecture](Decoder-Only.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Large pre-trained models like GPT-4 and LLaMA 2 exhibit the ability to perform tasks such as classification, summarization, translation, etc., by leveraging the appropriate prompt.\n",
    "\n",
    "The large language models, such as those in the GPT family, undergo pre-training using the Causal Language Modeling objective. This means the model aims to predict the next word, while the attention mechanism can only attend to previous tokens on the left. This implies that the model can solely rely on the previous context to predict the next token and is unable to peek at future tokens, preventing any form of cheating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(…)ingface.co/gpt2/resolve/main/config.json: 100%|████████████████████████████| 665/665 [00:00<00:00, 3.45MB/s]\n",
      "model.safetensors: 100%|████████████████████████████████████████████████████| 548M/548M [00:53<00:00, 10.2MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2Model(\n",
      "  (wte): Embedding(50257, 768)\n",
      "  (wpe): Embedding(1024, 768)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      "  (h): ModuleList(\n",
      "    (0-11): 12 x GPT2Block(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPT2Attention(\n",
      "        (c_attn): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): GPT2MLP(\n",
      "        (c_fc): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (act): NewGELUActivation()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "gpt2 = AutoModel.from_pretrained(\"gpt2\")\n",
    "print(gpt2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The provided code illustrates how the pipeline can be used to incorporate the GPT2 model for text generation. It generates four different alternatives to complete the phrase \"This movie was a very.”\n",
    "Copy\n",
    "\n",
    "generator = pipeline(model=\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(…)gpt2/resolve/main/generation_config.json: 100%|█████████████████████████████| 124/124 [00:00<00:00, 263kB/s]\n",
      "(…)gingface.co/gpt2/resolve/main/vocab.json: 100%|████████████████████████| 1.04M/1.04M [00:00<00:00, 4.36MB/s]\n",
      "(…)gingface.co/gpt2/resolve/main/merges.txt: 100%|██████████████████████████| 456k/456k [00:00<00:00, 2.12MB/s]\n",
      "(…)face.co/gpt2/resolve/main/tokenizer.json: 100%|████████████████████████| 1.36M/1.36M [00:00<00:00, 10.8MB/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">  fun and moving experience. The cast and crew were phenomenal. It's amazing how they got here in California.\n",
      "\n",
      "The cast of The Office went on a run that lasted about 18 hours. During that period of time, all the actors got their\n",
      ">  much experimental period piece that was very well received in the United States. It's one that I really enjoyed growing up and was very enjoyable. There was so much good, interesting stuff going on.\n",
      "\n",
      "\"I think our story was very strong in\n",
      ">  popular one. And my husband and I thought it was a great movie because there were so many different genres. And that brought up different things. And we realized, \"Well, our experience with this film was that I wasn't happy about it.\n",
      ">  much part of my life, and when you saw it for the first time, it was kind of like, \"Oh, my God. This guy was kind of great, was really good.\" It was like a great family story.\" The film really\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline(model=\"gpt2\")\n",
    "output = generator(\"This movie was a very\", do_sample=True, top_p=0.95, num_return_sequences=4, max_new_tokens=50, return_full_text=False)\n",
    "\n",
    "for item in output:\n",
    "  print(\">\", item['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.10-LLMs",
   "language": "python",
   "name": "py3.10-llms"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
