{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, dotenv_values\n",
    "import os\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv() \n",
    "# logging.basicConfig(filename=f'logging/eoddata_{datetime.today().date()}.log', filemode='a', format='%(asctime)s - %(message)s', datefmt='%d-%b-%y %H:%M:%S')\n",
    "# logging.getLogger().setLevel(logging.DEBUG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('ACTIVELOOP_TOKEN',\n",
       "              'eyJhbGciOiJIUzUxMiIsImlhdCI6MTY5NzE2ODA2NSwiZXhwIjoxNzI4NzkwNDQ3fQs.eyJpZCI6InRob21hc2Nob2kifQ._3bY2jeR20Mqqt94VR6nfxSurZFUAGeCwPzTQETfGB0FSC7R76S4WL_MRTc9T0b1ndis-J-Pf6DcDlsppxF5yA'),\n",
       "             ('OPENAI_API_KEY',\n",
       "              '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')])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = dotenv_values(\".env\")\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'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'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
    "OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# English text to translate\n",
    "english_text = \"Hello, how are you?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bonjour, comment √ßa va ?\n"
     ]
    }
   ],
   "source": [
    "openai.api_key = OPENAI_API_KEY\n",
    "response  = openai.ChatCompletion.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": f'Translate the following English text to French: \"{english_text}\"'}\n",
    "  ],\n",
    ")\n",
    "\n",
    "print(response['choices'][0]['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few-Shot Learning\n",
    "\n",
    "Few-shot learning in the context of LLMs refers to providing the model with a few examples before making predictions. These examples \"teach\" the model how to reason and act as \"filters\" to help the model search for relevant patterns in the dataset.\n",
    "\n",
    "The idea of few-shot learning is fascinating as it suggests that the model can be quickly reprogrammed for new tasks. While LLMs like GPT3 excel at language modeling tasks like machine translation, they may struggle with more complex reasoning tasks.\n",
    "\n",
    "The few-shot examples are helping the model search for relevant patterns in the dataset. The dataset, which is effectively compressed into the model's weights, can be searched for patterns that strongly respond to these provided examples. These patterns are then used to generate the model's output. The more examples provided, the more precise the output becomes.\n",
    "\n",
    "Here‚Äôs an example of few-shot learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt for summarization\n",
    "prompt = \"\"\"\n",
    "Describe the following movie using emojis.\n",
    "\n",
    "{movie}: \"\"\"\n",
    "\n",
    "examples = [\n",
    "\t{ \"input\": \"Titanic\", \"output\": \"üõ≥Ô∏èüåä‚ù§Ô∏èüßäüé∂üî•üö¢üíîüë´üíë\" },\n",
    "\t{ \"input\": \"The Matrix\", \"output\": \"üï∂Ô∏èüíäüí•üëæüîÆüåÉüë®üèª‚ÄçüíªüîÅüîìüí™\" }\n",
    "]\n",
    "\n",
    "movie = \"Toy Story\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nDescribe the following movie using emojis.\\n\\n{movie}: '"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nDescribe the following movie using emojis.\\n\\nTitanic: '"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " prompt.format(movie=examples[0][\"input\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß∏üöÄü§†üêçüë¶üßíüóùÔ∏èüíîüåà‚ú®\n"
     ]
    }
   ],
   "source": [
    "response = openai.ChatCompletion.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt.format(movie=examples[0][\"input\"])},\n",
    "        {\"role\": \"assistant\", \"content\": examples[0][\"output\"]},\n",
    "        {\"role\": \"user\", \"content\": prompt.format(movie=examples[1][\"input\"])},\n",
    "        {\"role\": \"assistant\", \"content\": examples[1][\"output\"]},\n",
    "        {\"role\": \"user\", \"content\": prompt.format(movie=movie)},\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(response['choices'][0]['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling Laws\n",
    "\n",
    "Scaling laws refer to the relationship between the model's performance and factors such as the number of parameters, the size of the training dataset, the compute budget, and the network architecture. They were discovered after a lot of experiments and are described in the Chinchilla paper. These laws provide insights into how to allocate resources when training these models optimally.\n",
    "\n",
    "The main elements characterizing a language model are:\n",
    "\n",
    "1. The number of parameters (N) reflects the model's capacity to learn from data. More parameters allow the model to capture complex patterns in the data.\n",
    "2.    The size of the training dataset (D) is measured in the number of tokens (small pieces of text ranging from a few words to a single character).\n",
    "3.  FLOPs (floating point operations per second) measure the compute budget used for training.\n",
    "\n",
    "The researchers trained the Chinchilla model, which has 70B parameters, on 1.4 trillioan tokens. This aligns with the rule of thumb proposed in the paper:  for a model with X parameters, it is optimal to train it on approximately X * 20 tokens. For example, in the context of this rule, a model with 100 billion parameters would be optimally trained on approximately 2 trillion tokens. \n",
    "\n",
    "Applying this rule, the Chinchilla model, though smaller, performed better than other LLMs. It showed gains in language modeling and task performance and needed less memory and computing power. You can read more about Chinchilla in its paper ‚ÄúTraining Compute-Optimal Large Language Models‚Äù."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emergent Abilities in LLMs\n",
    "\n",
    "Emergent abilities in LLMs refer to the sudden appearance of new capabilities as the size of the model increases. These abilities, which include performing arithmetic, answering questions, summarizing passages, and more, are not explicitly trained in the model. Instead, they seem to arise spontaneously as the model scales, hence the term \"emergent.\"\n",
    "\n",
    "    LLMs are probabilistic models that learn patterns in natural language. When these models are scaled up, they not only improve quantitatively in their ability to learn patterns, but they also exhibit qualitative changes in their behavior.\n",
    "\n",
    "Traditionally, the models require task-specific fine-tuning and architectural modifications to perform specific tasks. However, when scaled, these models can perform these tasks without any architectural modifications or task-specific training. They can do this simply by phrasing the tasks in terms of natural language. This capability of LLMs to perform tasks without fine-tuning is remarkable in itself.\n",
    "\n",
    "What's even more intriguing is how these abilities appear. As LLMs grow, they rapidly and unpredictably transition from near-zero to sometimes state-of-the-art performance. This phenomenon suggests that these abilities are emergent properties of the model's scale rather than being explicitly programmed into the model.\n",
    "\n",
    "This concept of emergent abilities in LLMs has significant implications for the field of AI, as it suggests that scaling up models can lead to the spontaneous development of new capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Transformer\n",
    "\n",
    "The most crucial model of the previous pipeline is, without doubt, the Transformer, introduced in the very popular paper ‚ÄúAttention Is All You Need.‚Äù The Transformer is a type of neural network that is used today by all of the best Large Language Models like GPT-4, Claude, and LLaMA. \n",
    "\n",
    "Central to Transformers is the encoder-decoder structure, which excels at modeling long-range dependencies and capturing contextual information.\n",
    "\n",
    "The **Encoder** processes the input text, identifying key elements and creating word embeddings based on their relevance to other words in the sentence. In the original Transformer architecture, designed for text translation, the attention mechanism was employed in two distinct ways: encoding the source language and decoding the encoded embedding back into the target language.\n",
    "\n",
    "On the other hand, the **Decoder** takes the encoder's output, an embedding, and transforms it back into text. Some models may opt to use only the decoder, bypassing the encoder entirely. The decoder's attention mechanism differs slightly from the encoder's, functioning more like a conventional language model by focusing on previous words during text processing. This approach is particularly useful for tasks like language generation, which is why models like GPT, primarily designed for text generation in response to an input text sequence, utilize the decoder part of the Transformer.\n",
    "\n",
    "Later in the course, we‚Äôll learn more about the Transformer architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to dive deeper into these models, I suggest reading the paper ‚ÄúA Survey of Large Language Models.‚Äù Here‚Äôs a table summarizing the architectural and training details of all the mentioned models (and others)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Benchmarks for Emergent Abilities\n",
    "\n",
    "Several benchmarks are used to evaluate the emergent abilities of language models. These include the BIG-Bench suite, TruthfulQA, the Massive Multi-task Language Understanding (MMLU) benchmark, and the Word in Context (WiC) benchmark.\n",
    "\n",
    "1. The first of these is the BIG-Bench suite, a comprehensive set of over 200 benchmarks that test a model's capabilities across a variety of tasks. These tasks include arithmetic operations where the model is expected to perform the four basic operations (example: ‚ÄúQ: What is 132 plus 762? A: 894), transliteration from the International Phonetic Alphabet (IPA) to measure if the model is able to manipulate and use rare words (example: ‚ÄúEnglish: The 1931 Malay census was an alarm bell. IPA: √∞…ô 1931 Ààme…™le…™ Ààs…õns…ôs w…ëz …ôn …ôÀàl…ërm b…õl.‚Äù), word unscrambling that analyzes the model‚Äôs ability to work with alphabets. A large number of benchmarks can be found within the Github repository where you can delve into their specific details. The performance of models like GPT-3 and LaMDA on these tasks starts near zero but jumps to significantly above random at a certain scale, demonstrating emergent abilities.\n",
    "2. Another benchmark is TruthfulQA, which measures a model's capacity to provide truthful responses when addressing questions. The evaluation consists of two tasks: 1) Generation: The model will be asked to answer a question with 1 or 2 sentences. 2) Multiple-choices: The second task involves multiple-choice questions, where the model must choose the correct answer from either 4 options or True/False statements. When the Gopher model is scaled up to its largest size, its performance jumps to more than 20% above random, indicating the emergence of this ability.\n",
    "3. The Massive Multi-task Language Understanding (MMLU) is another key benchmark. The primary objective of this benchmark is to evaluate models for their ability to demonstrate a broad range of world knowledge and problem-solving skills. The test encompasses 57 tasks, spanning areas such as elementary mathematics, US history, computer science, law, and more. GPTs, Gopher, and Chinchilla models of a specific scale do not perform better than guessing on average of all the topics, but scaling up to a larger size enables performance to surpass random, indicating the emergence of this ability.\n",
    "5. Finally, the Word in Context (WiC) is a semantic understanding benchmark. WiC is a binary classification task for context-sensitive word embeddings. It involves target words (verbs or nouns) with two provided contexts, aiming to determine if they share the same meaning. Chinchilla fails to achieve the one-shot performance of better than random, even when scaled to its largest model size. Above-random performance eventually emerged when PaLM was scaled to a much larger size, suggesting the emergence of this ability at a larger scale.\n",
    "\n",
    "## Other Factors That Could Give Rise To Emergent Abilities\n",
    "\n",
    "* Multi-step reasoning is a strategy where a model is guided to produce a sequence of intermediate steps before giving the final answer. This strategy, known as chain-of-thought prompting, only surpasses standard prompting when applied to a sufficiently large model.\n",
    "* Instruction following is another strategy that involves fine-tuning a model on a mixture of tasks phrased as instructions. This strategy only improves performance when applied to a model of a specific size.\n",
    "\n",
    "## Risks With Emergent Abilities\n",
    "\n",
    "As we scale up language models, we also need to be aware of the emergent risks that come with it. These risks could be societal issues related to truthfulness, bias, and toxicity. These risks can be avoided by applying strategies, such as giving model prompts that encourage them to be \"helpful, harmless, and honest.‚Äù\n",
    "\n",
    "The WinoGender benchmark, which measures gender bias in occupations, has shown that scaling can improve performance but also increase bias in ambiguous contexts. Larger models were found to be more likely to memorize training data, although deduplication methods can reduce this risk.\n",
    "\n",
    "Emergent risks also include phenomena that might only exist in future language models or that have not yet been characterized in current models. These could include backdoor vulnerabilities or harmful content synthesis.\n",
    "A Shift Towards General-Purpose Models\n",
    "\n",
    "The emergence of abilities has led to sociological changes in how the community views and uses these models. Historically, NLP focused on task-specific models. Scaling models has led to an explosion in research on \"general purpose\" models that aim to perform a range of tasks not explicitly encoded in the training data.\n",
    "\n",
    "This shift towards general-purpose models is evident when scaling enables a few-shot prompted general-purpose model to outperform prior state-of-the-art held by fine-tuned task-specific models. For example, GPT-3 achieved a new state-of-the-art on the TriviaQA and PiQA question-answering benchmarks; PaLM achieved a new state-of-the-art on three arithmetic reasoning benchmarks; and the multimodal Flamingo model achieved a new state of the art on six visual question answering benchmarks.\n",
    "\n",
    "The ability of general-purpose models to perform unseen tasks, given only a few examples, has also led to many new applications of language models outside the NLP research community. For instance, language models have been used by prompting to translate natural language instructions into actions that are executable by robots, interact with users, and facilitate multi-modal reasoning.\n",
    "## Conclusion\n",
    "\n",
    "Emergent abilities in LLMs are capabilities that appear as the models scale up and are a key factor in their success. These abilities, unpredictable from smaller models, become evident after reaching a certain scale threshold. They have been observed in various contexts, such as in a few-shot prompting and augmented prompting strategies. Scaling up LLMs also introduces emergent risks like increased bias and toxicity, which can be avoided with appropriate strategies. The emergence of these abilities has led to a shift towards general-purpose models and opened up new applications outside the traditional NLP research community. \n",
    "\n",
    "In the next lesson, we‚Äôll dive into today's most popular proprietary LLMs and describe the tradeoffs between proprietary and open-source LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Deep learning is a type of artificial intelligence that enables computers to learn and improve over time, based on data and experience. It is a subfield of machine learning, which focuses on the development of algorithms and models that can learn from and make predictions on data.\n",
      "\n",
      "Deep learning works by combining layers of artificial neural networks, which are designed to mimic the structure and function of the human brain. These neural networks are composed of many interconnected \"neurons,\" which can process and transmit information. The first layer of neurons receives input data and passes the information through the network, with each subsequent layer processing and transforming the data further. \n",
      "\n",
      "The final layer of neurons produces an output, such as a prediction or classification. Deep learning models are trained using large amounts of data and algorithms that adjust the connections between the neurons to improve the accuracy of the output. \n",
      "\n",
      "Deep learning has been successful in a wide range of applications, including image and speech recognition, natural language processing, and autonomous driving. It has\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import cohere\n",
    "import os\n",
    "\n",
    "co = cohere.Client(os.environ[\"COHERE_API_KEY\"])\n",
    "\n",
    "response = co.generate(\n",
    "    prompt='Please briefly explain to me how Deep Learning works using at most 100 words.',\n",
    "    max_tokens=200\n",
    ")\n",
    "print(response.generations[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.10-LLMs",
   "language": "python",
   "name": "py3.10-llms"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
